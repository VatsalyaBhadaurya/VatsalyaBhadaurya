<h1 align="center">Hi, Iâ€™m Vatsalya ğŸ‘‹</h1>

<p align="center">
  <img src="https://media.giphy.com/media/QNFhOolVeCzPQ2Mx85/giphy.gif" width="180"/>
</p>

<p align="center">
<b>Applied Robotics</b> Â· <b>Embodied AI</b> Â· <b>Humanoid Perception</b> Â· <b>Edge ML</b>
</p>

<p align="center">
<a href="https://vatsalyaaa.me"><b>Portfolio</b></a> Â·
<a href="https://www.linkedin.com/in/vatsalya-bhadaurya"><b>LinkedIn</b></a> Â·
<a href="mailto:vatbhadaurya@gmail.com"><b>Email</b></a>
</p>

---

## ğŸ§  About Me

<img align="right" src="https://media.giphy.com/media/coxQHKASG60HrHtvkt/giphy.gif" width="220"/>

I build **real-world AI systems for robotics**, focusing on **humanoid perception, behavior intelligence, and embodied decision-making**.

I care less about benchmarks and more about whether a system:
- survives noisy sensors  
- runs on limited compute  
- behaves reliably outside the lab  

Currently, Iâ€™m an **AI Intern in the R&D team at a humanoid robotics startup**, working on:
- perception pipelines  
- speech â†’ intent â†’ action systems  
- behavior logic & state machines  
- ROS2 architectures  
- edge deployment on **NVIDIA Jetson**

Iâ€™ve also worked on **industrial vision systems in automotive manufacturing** and co-authored an **IEEE research paper** on multimodal sensing.

---

## ğŸ”¬ Current Focus

<p align="center">
  <img src="https://media.giphy.com/media/l0HlNaQ6gWfllcjDO/giphy.gif" width="500"/>
</p>

- Humanoid perception & behavior systems  
- Embodied AI under uncertainty  
- Vision-Language-Action models  
- Edge ML optimization on Jetson  
- Robotics AI built for real-world deployment  

---

## ğŸ—ï¸ Experience Highlights

<img align="left" src="https://media.giphy.com/media/26tn33aiTi1jkl6H6/giphy.gif" width="200"/>

- AI Intern â€” Humanoid Robotics R&D (real robots, deployed systems)  
- Industrial humanoid vision system for alloy & paint defect detection (TVS Automobiles)  
- IEEE research on real-time multimodal microplastic detection  
- Founder â€” **MaaKosh** (maternal & neonatal health devices)  
- Arduino Community Speaker (international representation)  
- Multiple national-level hackathon wins (AI Â· IoT Â· Robotics)  

---

## âš™ï¸ Tech Stack

### ğŸ¤– Embodied AI & ML
<img align="right" src="https://media.giphy.com/media/3o7btPCcdNniyf0ArS/giphy.gif" width="220"/>

- Vision-Language-Action models  
- YOLOv8, DINOv3, SAM 2.1, Depth Anything, DUSt3R  
- Whisper, VAD, speech-to-intent grounding  
- TensorFlow, OpenCV, GPT-OSS, LLaVA-style architectures  

### ğŸš€ Edge & Systems
- Jetson Orin Nano, Jetson Nano  
- On-device inference optimization  
- Dockerized deployment pipelines  
- Linux-based robotics systems  

### ğŸ”§ Hardware
- STM32, Arduino, Raspberry Pi  
- IMU, RGB & depth cameras  
- Sensor fusion & low-level interfacing  

---

## ğŸ“„ Research

<img align="right" src="https://media.giphy.com/media/13HgwGsXF0aiGY/giphy.gif" width="200"/>

**Development of a Real-Time Multimodal Sensor for Field-Based Microplastic Detection**  
*IEEE Conference â€” IC3ECSBHI*

Multimodal fusion of **optical sensing**, **impedance analysis**, and **machine learning** for field-deployable environmental monitoring.

---

<p align="center">
  <img src="https://media.giphy.com/media/26ufdipQqU2lhNA4g/giphy.gif" width="300"/>
</p>

<p align="center">
<i>Building robots that perceive, reason, and act â€” outside the lab.</i>
</p>
