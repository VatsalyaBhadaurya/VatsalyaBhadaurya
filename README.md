<h1 align="center">Hi, Iâ€™m Vatsalya ğŸ‘‹</h1>

<p align="center">
  <img src="https://media.giphy.com/media/xTiTnHXbRoaZ1B1Mo8/giphy.gif" width="260"/>
</p>

<p align="center">
  <b>Applied Robotics</b> Â· <b>Embodied AI</b> Â· <b>Humanoid Perception</b> Â· <b>Edge ML</b>
</p>

<p align="center">
  <a href="https://vatsalyaaa.me"><b>Portfolio</b></a> Â·
  <a href="https://www.linkedin.com/in/vatsalya-bhadaurya"><b>LinkedIn</b></a> Â·
  <a href="mailto:vatbhadaurya@gmail.com"><b>Email</b></a>
</p>

---

<p align="center">
  <img src="https://media.giphy.com/media/l3vR85PnGsBwu1PFK/giphy.gif" width="520"/>
</p>

## ğŸ§  About Me

<p align="center">
I build <b>real-world AI systems for robotics</b>, with a focus on  
<b>humanoid perception, behavior intelligence, and embodied decision-making</b>.
</p>

<p align="center">
My work sits at the intersection of applied machine learning,  
robotics systems engineering, and on-device deployment â€”  
where models must survive noisy sensors, limited compute,  
and real environments, not just simulations.
</p>

<p align="center">
I am currently an <b>AI Intern in the R&D team at a humanoid robotics startup</b>,  
working on perception pipelines, speech â†’ intent â†’ action systems,  
behavior logic, ROS2 architectures, and edge deployment on  
<b>NVIDIA Jetson platforms</b>.
</p>

---

<p align="center">
  <img src="https://media.giphy.com/media/26tn33aiTi1jkl6H6/giphy.gif" width="520"/>
</p>

## ğŸ”¬ Current Focus

<p align="center">
Humanoid perception & behavior systems  
<br/>
Embodied AI and decision-making under uncertainty  
<br/>
Visionâ€“Languageâ€“Action & multimodal models  
<br/>
Edge ML optimization on Jetson platforms  
<br/>
Robotics AI designed for real-world deployment
</p>

---

<p align="center">
  <img src="https://media.giphy.com/media/3oKIPEqDGUULpEU0aQ/giphy.gif" width="520"/>
</p>

## ğŸ—ï¸ Experience Highlights

<p align="center">
â€¢ AI Intern â€” Humanoid Robotics R&D (real robots, deployed systems)  
<br/>
â€¢ Industrial humanoid vision system for alloy & paint defect detection (TVS Automobiles)  
<br/>
â€¢ IEEE research on real-time multimodal microplastic detection  
<br/>
â€¢ Founder â€” <b>MaaKosh</b>, maternal & neonatal health initiative  
<br/>
â€¢ Arduino Community Speaker (international representation)  
<br/>
â€¢ Multiple national-level hackathon wins (AI Â· IoT Â· Robotics)
</p>

---

<p align="center">
  <img src="https://media.giphy.com/media/f3iwJFOVOwuy7K6FFw/giphy.gif" width="520"/>
</p>

## âš™ï¸ Tech Stack

<p align="center"><b>Embodied AI & Machine Learning</b></p>

<p align="center">
Visionâ€“Languageâ€“Action models Â· Visionâ€“Language Models Â· Multimodal models  
<br/>
YOLOv8 Â· DINOv3 Â· SAM 2.1 Â· Depth Anything Â· DUSt3R  
<br/>
Whisper Â· Voice Activity Detection Â· Speech-to-Intent Grounding  
<br/>
TensorFlow Â· OpenCV Â· GPT-OSS Â· LLaVA-style architectures
</p>

<br/>

<p align="center"><b>Edge AI & Systems Engineering</b></p>

<p align="center">
Jetson Orin Nano Â· Jetson Nano  
<br/>
On-device inference optimization (latency & memory constrained)  
<br/>
Dockerized deployment pipelines  
<br/>
Linux-based robotics systems
</p>

<br/>

<p align="center"><b>Robotics Hardware & Embedded</b></p>

<p align="center">
STM32 Â· Arduino Â· Raspberry Pi  
<br/>
IMU Â· RGB & depth cameras Â· environmental sensors  
<br/>
Low-level sensor integration & hardwareâ€“software interfacing
</p>

---

<p align="center">
  <img src="https://media.giphy.com/media/xTiTnxpQ3ghPiB2Hp6/giphy.gif" width="520"/>
</p>

## ğŸ“„ Research

<p align="center">
<b>Development of a Real-Time Multimodal Sensor for Field-Based Microplastic Detection</b>  
<br/>
<i>IEEE Conference â€” IC3ECSBHI</i>
</p>

<p align="center">
Multimodal sensor fusion combining optical sensing,  
impedance analysis, and machine learning for  
real-time, field-deployable environmental monitoring.
</p>

---

<p align="center">
  <img src="https://media.giphy.com/media/3o7qE1YN7aBOFPRw8E/giphy.gif" width="340"/>
</p>

<p align="center">
<i>Building robots that perceive, reason, and act â€” outside the lab.</i>
</p>
