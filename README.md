<h1 align="center">Hi, Iâ€™m Vatsalya ğŸ‘‹</h1>

<p align="center">
  <img src="https://media.giphy.com/media/L8K62iTDkzGX6/giphy.gif" width="220"/>
</p>

<p align="center">
<b>Applied Robotics</b> Â· <b>Embodied AI</b> Â· <b>Humanoid Perception</b> Â· <b>Edge ML</b>
</p>

<p align="center">
<a href="https://vatsalyaaa.me"><b>Portfolio</b></a> Â·
<a href="https://www.linkedin.com/in/vatsalya-bhadaurya"><b>LinkedIn</b></a> Â·
<a href="mailto:vatbhadaurya@gmail.com"><b>Email</b></a>
</p>

---

## ğŸ§  About Me

<p align="right">
  <img src="https://media.giphy.com/media/3oKIPEqDGUULpEU0aQ/giphy.gif" width="260"/>
</p>

I build **real-world AI systems for robotics**, with a focus on **humanoid perception, behavior intelligence, and embodied decision-making**.

My work prioritizes systems that:
- operate under sensor noise  
- respect compute and latency limits  
- behave robustly outside controlled environments  

I am currently an **AI Intern in the R&D team at a humanoid robotics startup**, working on:
- perception pipelines  
- speech â†’ intent â†’ action systems  
- behavior logic & state machines  
- ROS2-based architectures  
- edge deployment on **NVIDIA Jetson**

I have also worked on **industrial-grade vision systems in automotive manufacturing** and co-authored an **IEEE research paper** on multimodal sensing.

---

## ğŸ”¬ Current Focus

<p align="center">
  <img src="https://media.giphy.com/media/l46Cy1rHbQ92uuLXa/giphy.gif" width="520"/>
</p>

- Humanoid perception & behavior systems  
- Embodied AI under uncertainty  
- Visionâ€“Languageâ€“Action models  
- Edge ML optimization on Jetson  
- Robotics AI designed for real-world deployment  

---

## ğŸ—ï¸ Experience Highlights

<p align="left">
  <img src="https://media.giphy.com/media/26BRQTezZrKak4BeE/giphy.gif" width="240"/>
</p>

- AI Intern â€” Humanoid Robotics R&D (real robots, deployed systems)  
- Industrial humanoid vision system for alloy & paint defect detection (TVS Automobiles)  
- IEEE research on real-time multimodal microplastic detection  
- Founder â€” **MaaKosh**, maternal & neonatal health initiative  
- Arduino Community Speaker (international representation)  
- Multiple national-level hackathon wins (AI Â· IoT Â· Robotics)  

---

## âš™ï¸ Tech Stack

### ğŸ¤– Embodied AI & Machine Learning

<p align="right">
  <img src="https://media.giphy.com/media/f3iwJFOVOwuy7K6FFw/giphy.gif" width="260"/>
</p>

- Visionâ€“Languageâ€“Action models  
- YOLOv8, DINOv3, SAM 2.1, Depth Anything, DUSt3R  
- Whisper, voice activity detection, speech-to-intent grounding  
- TensorFlow, OpenCV, GPT-OSS, LLaVA-style architectures  

### ğŸš€ Edge AI & Systems Engineering

- NVIDIA Jetson Orin Nano, Jetson Nano  
- On-device inference optimization (latency & memory constrained)  
- Dockerized deployment pipelines  
- Linux-based robotics systems  

### ğŸ”§ Robotics Hardware & Embedded

- STM32, Arduino, Raspberry Pi  
- IMU, RGB & depth cameras, environmental sensors  
- Low-level sensor integration & hardwareâ€“software interfacing  

---

## ğŸ“„ Research

<p align="right">
  <img src="https://media.giphy.com/media/xTiTnxpQ3ghPiB2Hp6/giphy.gif" width="240"/>
</p>

**Development of a Real-Time Multimodal Sensor for Field-Based Microplastic Detection**  
*IEEE Conference â€” IC3ECSBHI*

This work focuses on multimodal sensor fusion combining **optical sensing**, **impedance analysis**, and **machine learning** to enable real-time, field-deployable environmental monitoring systems.

---

<p align="center">
  <img src="https://media.giphy.com/media/3o7qE1YN7aBOFPRw8E/giphy.gif" width="320"/>
</p>

<p align="center">
<i>Building robots that perceive, reason, and act â€” outside the lab.</i>
</p>
