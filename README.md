<h1 align="center">Hi, Iâ€™m Vatsalya ğŸ‘‹</h1>

<p align="center">
<b>Applied Robotics</b> Â· <b>Embodied AI</b> Â· <b>Humanoid Perception</b> Â· <b>Edge ML</b>
</p>

<p align="center">
<a href="https://vatsalyaaa.me"><b>Portfolio</b></a> Â·
<a href="https://www.linkedin.com/in/vatsalya-bhadaurya"><b>LinkedIn</b></a> Â·
<a href="mailto:vatbhadaurya@gmail.com"><b>Email</b></a>
</p>

---

## ğŸ§  About Me

I work on **real-world AI systems for robotics**, with a primary focus on **humanoid perception, behavior intelligence, and embodied decision-making**.

My work lives at the intersection of:
- applied machine learning  
- robotics systems engineering  
- on-device deployment  

where models must function **reliably under real constraints** â€” noisy sensors, limited compute, latency budgets, and unpredictable environments â€” not just controlled simulations.

I am currently an **AI Intern in the R&D team at a humanoid robotics startup**, working directly on:
- perception pipelines  
- speech â†’ intent â†’ action systems  
- behavior logic and state machines  
- ROS2-based architectures  
- edge deployment on **NVIDIA Jetson** platforms  

Previously, Iâ€™ve worked on **industrial-grade vision systems deployed in automotive manufacturing** and co-authored an **IEEE research paper** on multimodal sensing systems.

---

## ğŸ”¬ Current Focus

- Humanoid perception and behavior systems  
- Embodied AI and decision-making under uncertainty  
- Vision-Language-Action and multimodal models  
- Edge ML optimization on Jetson platforms  
- Robotics AI systems built for real-world deployment  

---

## ğŸ—ï¸ Experience Highlights

- AI Intern â€” Humanoid Robotics R&D (real robots, deployed systems)  
- Industrial humanoid vision system for **alloy defect & paint anomaly detection** (TVS Automobiles)  
- IEEE research on **real-time multimodal microplastic detection**  
- Founder â€” **MaaKosh**, maternal & neonatal health initiative (medical devices + AI)  
- Arduino Community Speaker with international representation  
- Multiple national-level hackathon wins across **AI, IoT, and robotics**  

---

## âš™ï¸ Tech Stack

### ğŸ¤– Embodied AI & Machine Learning
- Vision-Language-Action models  
- Vision-Language Models & large multimodal models  
- YOLOv8, DINOv3, SAM 2.1, Depth Anything, DUSt3R  
- Whisper, voice activity detection, speech-to-intent grounding  
- TensorFlow, OpenCV, GPT-OSS, LLaVA-style architectures  

### ğŸš€ Edge AI & Systems Engineering
- NVIDIA Jetson Orin Nano, Jetson Nano  
- On-device inference optimization (latency & memory constrained)  
- Dockerized deployment pipelines  
- Linux-based robotics systems  

### ğŸ”§ Robotics Hardware & Embedded
- STM32, Arduino, Raspberry Pi  
- IMU, RGB & depth cameras, environmental sensors  
- Low-level sensor integration & hardware-software interfacing  

---

## ğŸ“„ Research

**Development of a Real-Time Multimodal Sensor for Field-Based Microplastic Detection**  
*IEEE Conference â€” IC3ECSBHI*

Multimodal sensor fusion combining **optical sensing**, **impedance analysis**, and **machine learning** to enable real-time, field-deployable environmental monitoring systems.

---

<p align="center">
<i>Building robots that perceive, reason, and act â€” outside the lab.</i>
</p>
